{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52da846f",
   "metadata": {},
   "source": [
    "## 2. 퍼셉트론\n",
    "\n",
    "<img src=\"img/deep_learning_images/fig_2-1.png\" width=\"224\" height=\"224\">\n",
    "\n",
    "입력신호 $x_1, x_2$에 가중치 $w_1, w_2$가 각각 곱해지고, 이들을 모두 더한 값이 임계값 $\\theta$보다 크면 1을 반환하는 원리.\n",
    "\n",
    "가중치는 각 신호가 결과에 주는 영향력을 조절하는 요소이기에, 특정 가중치의 값이 더 크다면, 그에 해당하는 입력 신호가 더 중요하다는 것.\n",
    "\n",
    "<img src=\"img/deep_learning_images/perceptron.png\" width=\"512\" height=\"512\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7af6d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ffc53ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x1, x2):\n",
    "    w1 = random.uniform(0.1, 0.9)\n",
    "    w2 = random.uniform(0.1, 0.9)\n",
    "    theta = random.uniform(0.1, 0.9)\n",
    "    \n",
    "    total = (x1 * w1) + (x2 * w2)\n",
    "    if total >= theta:\n",
    "        return 1\n",
    "        \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a893ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2 = random.randint(0, 1), random.randint(0, 1)\n",
    "perceptron(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd91ef7",
   "metadata": {},
   "source": [
    "### [Reference](https://sacko.tistory.com/10)\n",
    "\n",
    "<img src=\"img/deep_learning_images/e_3.1.png\" width=\"360\" height=\"360\">\n",
    "\n",
    "처음에 봤던 수식에서 $\\theta$를 $-b$로 치환한 것으로 bias, 편향이라고 부른다.\n",
    "\n",
    "입력 신호에 가중치를 곱한 값과 편향을 합해서 그 값이 0을 넘으면 1을 출력하고 그렇지 않으면 0을 출력한다.\n",
    "\n",
    "- ***가중치는 각 입력 신호가 결과에 주는 영향력(중요도)를 조절하는 매개변수.***\n",
    "- ***편향은 뉴런이 얼마나 쉽게 활성화하느냐를 조정하는 매개변수.***\n",
    "\n",
    "편향은 θ(theta)로 학습 데이터(Input)이 가중치와 계산되어 넘어야 하는 임계점으로 이 값이 높으면 높을 수록 그만큼 분류의 기준이 엄격하다는 것을 의미한다.\n",
    "\n",
    "그래서 편향이 높을 수록 모델이 간단해지는 경향이 있으며 (변수가 적고 더 일반화 되는 경우) 오히려 과소적합(underfitting)의 위험이 발생하게 된다.\n",
    "\n",
    "반대로 편향이 낮을수록 한계점이 낮아 데이터의 허용범위가 넓어지는 만큼 학습 데이터에만 잘 들어맞는 모델이 만들어질 수 있으며 모델이 더욱 복잡해질 것이다. 허용범위가 넓어지는 만큼 필요 없는 노이즈가 포함될 가능성도 높다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8732fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0, 1]) # input\n",
    "w = np.array([0.5, 0.5]) # weights\n",
    "b = -0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a4ea70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "594b26cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.19999999999999996"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(x * w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b867f",
   "metadata": {},
   "source": [
    "### AND gate\n",
    "\n",
    "<img src=\"img/deep_learning_images/fig_2-2.png\" width=\"360\" height=\"360\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bf73143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([0.5, 0.5])\n",
    "    b = -0.7\n",
    "    \n",
    "    tmp = sum(w * x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b5349a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(AND(0, 0))\n",
    "print(AND(1, 0))\n",
    "print(AND(0, 1))\n",
    "print(AND(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c35615",
   "metadata": {},
   "source": [
    "### OR, NAND\n",
    "\n",
    "<img src=\"img/deep_learning_images/fig_2-3.png\" width=\"360\" height=\"360\">\n",
    "<img src=\"img/deep_learning_images/fig_2-4.png\" width=\"360\" height=\"360\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f655d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([0.5, 0.5])\n",
    "    b = -0.2\n",
    "    \n",
    "    tmp = np.sum(w * x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def NAND(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([-0.5, -0.5])\n",
    "    b = 0.7\n",
    "\n",
    "    tmp = np.sum(w * x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0115ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(OR(0, 0))\n",
    "print(OR(1, 0))\n",
    "print(OR(0, 1))\n",
    "print(OR(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb4a66",
   "metadata": {},
   "source": [
    "<img src=\"img/deep_learning_images/fig_2-6.png\" width=\"360\" height=\"360\">\n",
    "\n",
    "위 그래프는 OR그래프를 그린 것으로, 0을 출력하는 영역이 회색으로 표기되어 있다.  \n",
    "즉, 0이 출력되는 경우와 1이 출력되는 경우가 명확하게 구분된다는 것이다.\n",
    "\n",
    "반면에 XOR게이트를 그려보면 어떨지 보자.\n",
    "<img src=\"img/deep_learning_images/fig_2-5.png\" width=\"224\" height=\"224\">\n",
    "<img src=\"img/deep_learning_images/fig_2-7.png\" width=\"360\" height=\"360\">\n",
    "\n",
    "그림에서 보이듯, XOR게이트는 직선으로 0이 출력되는 경우와 1이 출력되는 경우를 구분할 수가 없다.  \n",
    "따라서 비선형인 곡선을 그려야 두 경우를 구분할 수 있다.\n",
    "<img src=\"img/deep_learning_images/fig_2-8.png\" width=\"360\" height=\"360\">\n",
    "\n",
    "이를 위해서는 퍼셉트론 하나로는 불가능하기 때문에, 다층 퍼셉트론의 개념을 접목해야한다.\n",
    "<img src=\"img/deep_learning_images/fig_2-11.png\" width=\"360\" height=\"360\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79c0a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR(x1, x2):\n",
    "    result1 = OR(x1, x2)\n",
    "    result2 = NAND(x1, x2)\n",
    "    result3 = AND(result1, result2)\n",
    "    \n",
    "    return result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b8129ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(XOR(0, 0))\n",
    "print(XOR(1, 0))\n",
    "print(XOR(0, 1))\n",
    "print(XOR(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3888f586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
