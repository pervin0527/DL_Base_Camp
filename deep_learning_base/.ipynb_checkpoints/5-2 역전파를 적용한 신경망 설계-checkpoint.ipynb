{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f79351ee",
   "metadata": {},
   "source": [
    "이제부터는 신경망을 구성하는 층 각각을 클래스 하나로 구현한다. 먼저 활성화 함수인 ReLU와 Sigmoid 계층을 구현해볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c576b013",
   "metadata": {},
   "source": [
    "## ReLU 계층\n",
    "ReLU 함수의 수식은 다음과 같다.\n",
    "<img src=\"img/deep_learning_images/e_5.7.png\" width=150 height=150>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fd2742",
   "metadata": {},
   "source": [
    "위 수식에서 x에 대한 y의 미분은\n",
    "<img src=\"img/deep_learning_images/e_5.8.png\" width=150 height=150>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a70ef0d",
   "metadata": {},
   "source": [
    "이를 통해 알 수 있는 점은 순전파의 입력인 x가 0보다 크면 역전파는 이전 노드의 값을 그대로 다음 노드로 흘린다.(1이 곱해지는 것이니까 값에 변동이 없음)  \n",
    "반면에 순전파 때 x가 0이하일 경우 역전파 때는 다음 노드로 신호를 보내지 않는다.(0을 보냄) 이를 그림으로 나타내보면,\n",
    "<img src=\"img/deep_learning_images/fig_5-18.png\" width=512 height=512>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407f81ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20fc09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None # True와 False로 구성된 넘파이 배열\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0) # 순전파의 입력인 x의 원소 값이 0 이하인 인덱스는 True, 0보다 큰 원소는 False로.\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0 # True인 경우 out의 원소는 0이 된다.\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c560bc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "702efece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "mask = (x <= 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b58c30a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 3.]]\n"
     ]
    }
   ],
   "source": [
    "test_relu_layer = Relu()\n",
    "print(test_relu_layer.forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb2be5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [ True False]]\n",
      "[[1. 0.]\n",
      " [0. 3.]]\n"
     ]
    }
   ],
   "source": [
    "mask = (x <= 0)\n",
    "print(mask)\n",
    "\n",
    "x[mask] = 0\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266888d6",
   "metadata": {},
   "source": [
    "## Sigmoid 계층\n",
    "<img src=\"img/deep_learning_images/e_5.9.png\" width=180 height=180>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222234a2",
   "metadata": {},
   "source": [
    "<img src=\"img/deep_learning_images/fig_5-19.png\" width=448 height=448>\n",
    "<img src=\"img/deep_learning_images/sigmoid_node.png\" width=416 height=416>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f8e63",
   "metadata": {},
   "source": [
    "먼저 나눗셈 노드에 대한 미분을 하면,\n",
    "<img src=\"img/deep_learning_images/e_5.10.png\" width=120 height=120>\n",
    "식에서 알 수 있듯 이전 노드에서 흘러온 값에 $-y^2$(순전파의 출력을 제곱하고, 마이너스를 붙인 값)을 곱해서 다음 노드로 전달한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e14aac0",
   "metadata": {},
   "source": [
    "exp 노드는 $y=exp(x)$연산인데 미분 결과는 다음과 같다.\n",
    "<img src=\"img/deep_learning_images/e_5.11.png\" width=150 height=150>\n",
    "\n",
    "즉, 이전 노드의 값에 순전파 때의 출력을 곱해(여기서는 $exp(-x)$) 다음 노드로 전달."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed868f5",
   "metadata": {},
   "source": [
    "<img src=\"img/deep_learning_images/fig_5-19(3).png\" width=448 height=448>\n",
    "<img src=\"img/deep_learning_images/fig_5-20.png\" width=448 height=448>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d176d1",
   "metadata": {},
   "source": [
    "여기서 역전파 값을 한 번 더 정리하면,\n",
    "<img src=\"img/deep_learning_images/e_5.12.png\" width=448 height=448>\n",
    "<img src=\"img/deep_learning_images/sigmoid_formula_reform.png\" width=256 height=256>\n",
    "\n",
    "정리한 식의 마지막에서 볼 수 있듯 순전파의 출력값만으로 역전파를 계산할 수 있음을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "454f1fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e37d59a",
   "metadata": {},
   "source": [
    "## Affine 계층\n",
    "신경망의 순전파 때 수행하는 행렬의 곱은 기하학에서 Affine transform이라고 한다.  \n",
    "행렬의 곱과 편향의 합을 계산 그래플 나타내보면 다음과 같다.(지금까지의 계산 그래프에서는 노드 사이에 스칼라값이었으나 이번에는 행렬이다.)\n",
    "<img src=\"img/deep_learning_images/fig_5-24.png\" height=448 width=448>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80993615",
   "metadata": {},
   "source": [
    "그렇다면 역전파는 어떨까? 행렬을 사용한 역전파도 행렬의 원소마다 전개해보면 스칼라값을 사용한 지금까지의 계산 그래프와 같은 순서로 생각할 수 있으며 이를 종합해 식으로 전개해보면\n",
    "<img src=\"img/deep_learning_images/e_5.13.png\" height=150 width=150>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1dc5d8",
   "metadata": {},
   "source": [
    "$W^T$의 T는 전치행렬을 뜻한다. 전치행렬은 W의 i번째 행, j번째 열에 해당하는 원소를 j번째 행, i번째 열로 위치를 바꾼 것이다.  \n",
    "(i, j) ---> (j, i) 다음과 같이 표현 가능.\n",
    "<img src=\"img/deep_learning_images/e_5.14.png\" height=224 width=224>\n",
    "행렬 W의 형상이 (2, 3)와 전치행렬 $W^T$의 형상 (3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a32d21",
   "metadata": {},
   "source": [
    "<img src=\"img/deep_learning_images/fig_5-25.png\" height=512 width=512>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883d8b34",
   "metadata": {},
   "source": [
    "주목해야할 점은 역전파때의 행렬 곱 연산에서는 일반적인 역전파의 곱셈 노드처럼 서로 값을 바꾸는 것만이 아닌,  \n",
    "행렬의 전치를 서로 바꿔서 곱한다는 점이다.\n",
    "<img src=\"img/deep_learning_images/fig_5-26.png\" width=448 height=448>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461bf087",
   "metadata": {},
   "source": [
    "지금까지의 Affine 연산에 대한 내용은 입력 데이터로 단일 값이 사용된 경우이다. 이번에는 데이터 N개를 묶었을 때를 다룬다.\n",
    "<img src=\"img/deep_learning_images/fig_5-27.png\" width=448 height=448>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ba6a7",
   "metadata": {},
   "source": [
    "입력 데이터인 행렬 X의 형상이 (N, 2)로 변경되었다. 계산그래프 상으로 보았을 때 과정은 별다른 차이가 없어보인다.  \n",
    "순전파 때 편향의 덧셈은 $X \\cdot W$에 대한 편향이 각 데이터에 더해지게 되는데, 예를 들어 N=2인 경우 편향은 그 두 데이터 각각에 더해지게 된다는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8f5e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "B = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f8d00b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "print(X_dot_W + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8cb7d6",
   "metadata": {},
   "source": [
    "보이는 것처럼 순전파의 편향 덧셈은 각각의 데이터에 더해진다. 그래서 역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34e8228a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "dB = np.sum(dY, axis=0)\n",
    "\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8a57f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Afffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f267fd96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
