{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf0a3464",
   "metadata": {},
   "source": [
    "## 경사법\n",
    "\n",
    "앞서 본 기록을 통해 현재 매개변수 값에 대한 손실함수의 기울기를 구해, 손실함수 값이 최소가 되는 방향으로 매개변수를 업데이트함을 배웠다.  \n",
    "사실 이것이 바로 경사법이다. 경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동하고, 이동한 장소에서 또 기울기를 구하고 이동함을 반복하는 방법이다.\n",
    "\n",
    "책에는 간략하게 나온 내용이며, 지금 당장 필요한가 싶은 내용이지만,  \n",
    "경사법이 100%로 손실함수 값을 최적으로 최소가 되도록 만든다는 보장은 없다. 매개변수 수가 엄청나게 많으며 손실함수가 복잡하기 때문에 최소값이 되는(기울기가 0이되는) 최적의 지역을 찾는 것이 쉽지 않다.\n",
    "\n",
    "<img src=\"img/deep_learning_images/local_vs_global.png\" width=320 height=320>  \n",
    "\n",
    "> 위 그림에서 local minima라고 표기된 지역에서 기울기는 0이 되지만, 실제로는 global minima라는 손실 함수 값이 더 최소가 되는 지역이 존재함.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dbc956",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "<img src=\"img/deep_learning_images/e_4.7.png\" height=224 width=224>\n",
    "\n",
    "기호 $\\eta$(에타)는 갱신하는 양을 나타내는 것으로, 이를 신경망 학습에서는 학습률-Learning Rate라고 한다.  \n",
    "한 번의 학습으로 매개변수 값을 얼마나 갱신하느냐를 의미.\n",
    "\n",
    "위 식은 1회에 해당하는 갱신을 수식으로 나타낸 것으로 이를 여러 번 반복하면서 서서히 손실 함수의 값을 줄인다.  \n",
    "학습률은 0.01이나 0.001 등 미리 특정한 값으로 정해두어야하며, 값이 너무 크거나 작으면 최적의 장소를 찾아갈 수 없다.\n",
    "\n",
    "### 참고 -  하이퍼 파라미터\n",
    "학습률과 같은 매개변수를 하이퍼파라미터라고 하는데, 가중치와 편향 같은 신경망의 매개변수와는 성질이 다른 매개변수이다.  \n",
    "신경망의 가중치 매개변수는 학습 과정에서 자동으로 확득되는 매개변수인 반면, 학습률 같은 하이퍼파라미터는 사람이 직접 설정해야 하는 매개변수를 뜻함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9458b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from gradient_2d import numerical_gradient\n",
    "from functions import cross_entropy_error, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a79ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86c580c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1] ** 2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73d51f0",
   "metadata": {},
   "source": [
    "초기값 (-3.0, 4.0)으로 설정하고 경사법을 사용해 최솟값 탐색을 진행한다. 결과 값은 -6.11110793e-10,  8.14814391e-10로 거의 (0, 0)에 가까운 결과다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9491b3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append(x.copy())\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])    \n",
    "\n",
    "lr = 0.1\n",
    "step_num = 20\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7db4654",
   "metadata": {},
   "source": [
    "## 신경망의 기울기\n",
    "4-2와 4-3에서 본 내용들을 종합해서 신경망에서 기울기를 구해보자.  \n",
    "수차례 언급했지만 신경망의 기울기는 가중치 매개변수$(x)$에 대한 손실함수$(f(x))$의 기울기를 구하는 것이다.\n",
    "\n",
    "<img src=\"img/deep_learning_images/e_4.8.png\" width=224 height=224>\n",
    "$W$는 가중치, $L$이 손실함수라고 하면, $\\frac {\\partial L}{\\partial W}$로 표기할 수 있고, 이는 각각의 원소에 대한 손실함수의 편미분을 의미한다.  \n",
    "\n",
    "(1행 1번째 원소인 $\\frac {\\partial w_{11}}{\\partial L}$은 $w_{11}$을 조금 변경했을 때 손실함수 L이 얼마나 변화하느냐를 의미.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334bb7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        # self.W = np.random.randn(2, 3)\n",
    "        self.W = np.array([[0.47355232, 0.9977393, 0.84668094], [0.85557411, 0.03563661, 0.69422093]])\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f7852e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47355232 0.9977393  0.84668094]\n",
      " [0.85557411 0.03563661 0.69422093]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93c6401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.6, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17e479d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.05414809 0.63071653 1.1328074 ]\n"
     ]
    }
   ],
   "source": [
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36d4751c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9280682857864075"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a207ddfe",
   "metadata": {},
   "source": [
    "- 입력값 $x = [0.6, 0.9]$\n",
    "- predict는 입력값과 가중치의 행렬 곱연산\n",
    "- loss는 입력값과 가중치 곱연산 결과에 softmax를 적용하고, 실제 정답과 예측간의 오차를 구해 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84fb60a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21924757  0.14356243 -0.36281   ]\n",
      " [ 0.32887136  0.21534364 -0.544215  ]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5c685",
   "metadata": {},
   "source": [
    "numerical_gradient 함수를 통해 입력값과 가중치를 곱한 값이 실제 데이터와 얼만큼의 차이를 보이는지 loss를 구한 다음, 이에 대한 편미분 연산을 한다.  \n",
    "위 결과를 예로 들면, \n",
    "\n",
    "- $\\frac {\\partial L}{\\partial W}$의 $\\frac {\\partial L}{\\partial W_{11}}$은 대략 0.2로 $w_{11}$을 h만큼 늘리면 손실 함수 값은 0.2h만큼 증가한다는 뜻. \n",
    "- $\\frac {\\partial L}{\\partial W_{23}}$ 은 -0.5이므로 $\\frac {\\partial L}{\\partial W_{23}}$을 h만큼 늘리면 손실 함수 값은 0.5h만큼 감소한다.\n",
    "- 따라서 손실 함수 값을 줄인다는 목표를 이루려면 $\\frac {\\partial L}{\\partial W_{11}}$ 은 음의 방향으로, $\\frac {\\partial L}{\\partial W_{23}}$은 양의 방향으로 갱신해야 함을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19539b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
