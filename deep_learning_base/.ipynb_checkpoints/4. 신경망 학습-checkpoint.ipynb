{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e510f4",
   "metadata": {},
   "source": [
    "## 신경망 학습\n",
    "\n",
    "신경망이 데이터에서 학습한다는 것은 가중치 매개변수(weight와 bias)의 값을 데이터를 보고 자동으로 결정한다는 뜻이다.  \n",
    "(우리가 신경망의 가중치 매개변수를 수작업으로 설정하지 않아도 되는 이유)  \n",
    "\n",
    "> 딥러닝을 종단간 기계학습 ***end-to-end machine learning*** 이라고도 하며, 이는 데이터 입력에서 목표한 결과를 **사람의 개입없이** 얻는다는 뜻이다.\n",
    "\n",
    "보통은 훈련 데이터와 시험 데이터를 나눠서 진행하는데, 우리가 궁극적으로 원하는 것이 범용적으로 사용할 수 있는 모델이기 때문이다.  \n",
    "범용 능력은 아직 보지 못한 데이터로도 문제를 올바르게 풀어내는 능력을 뜻한다.  \n",
    "\n",
    "### 오버 피팅\n",
    "손글자 숫자 인식의 최종 결과는 택배에서 우편 번호를 자동으로 판독하는 시스템에 쓰일지 모른다!  \n",
    "누구인지 모르는 수 많은 사람들이 쓴 글자를 인식하는 능력이 중요한 것.  \n",
    "즉, 범용성이 떨어지게 되면 훈련 데이터는 잘 판별하지만 테스트 데이터(실전)에서는 제대로 판별하지 못하는 경우를 ***오버피팅***이라 한다.\n",
    "\n",
    "### 손실 함수\n",
    "손실 함수는 신경망 성능의 나쁨을 나타내는 지표로, 현재 신경망이 훈련 데이터를 얼마나 잘처리하지 못하느냐를 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2bd01b",
   "metadata": {},
   "source": [
    "### 평균 제곱 오차 - Mean Squared Error(MSE)\n",
    "\n",
    "<img src=\"img/deep_learning_images/e_4.1.png\" height=224 width=224>\n",
    "\n",
    "$y_k$는 신경망이 추정한 값(출력값), $t_k$는 정답 레이블, k는 데이터의 차원수를 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7021d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "486615e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]) # softmax의 출력값, 인덱스 2가 가장 확률값이 높음\n",
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0]) # 정답은 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "704e3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * sum((y - t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cb2638d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62740578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5974999999999999"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]) # 출력값의 인덱스 7번이 가장 높음\n",
    "mean_squared_error(y2, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc28c83",
   "metadata": {},
   "source": [
    "위 코드에서 볼 수 있듯 y가 y2보다 손실 함수 값이 더 작으므로, ***오차가 더 작다로 볼 수 있다.(정답에 더 가깝다)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b141e68a",
   "metadata": {},
   "source": [
    "### 교체 엔트로피 - Cross Entropy Error\n",
    "<img src=\"img/deep_learning_images/e_4.2.png\" width=224 height=224>\n",
    "\n",
    "$log$는 밑이 $e$인 자연로그이며 $y_k$는 신경망의 출력, $t_k$는 정답레이블.  \n",
    "구현에서 delta값을 더해주는 이유는 ```np.log```함수는 0을 입력하면 마이너스 무한대를 뜻하는 -inf가 되어 더 이상 계산을 진행할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30b0fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -(sum(t * (np.log(y + delta))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25dd9817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "\n",
    "cross_entropy_error(y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0471796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22314342631421757"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test1 = np.array([0.1, 0.05, 0.8, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "cross_entropy_error(y_test1, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e187b7c3",
   "metadata": {},
   "source": [
    "수식에서의 $t_k$는 One-hot-encoding 된 벡터이기 때문에, 정답이 아닌 다른 인덱스는 0이고 예측값 벡터에 곱해지면 정답에 해당하는 인덱스의 확률값만 남게 된다.  \n",
    "즉, 정답일 때의 신경망 출력값의 자연로그를 계산하는 식이된다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fd7c519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.       , 0.3566748])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = np.array([0, 1])\n",
    "y2 = np.array([0.1, 0.7])\n",
    "delta2 = 1e-7\n",
    "\n",
    "-(t2 * (np.log(y2 + delta2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4dca19",
   "metadata": {},
   "source": [
    "이렇게 정답에 해당하는 출력(확률)이 커질수록 자연로그의 y값은 0에 다가가고, 출력이 1일 때는 y값은 0이된다.  \n",
    "또한 출력이 작아질수록 오차는 커진다.\n",
    "\n",
    "<img src=\"img/deep_learning_images/fig_4-3.png\" width=448 height=448>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10172564",
   "metadata": {},
   "source": [
    "위에서 본 손실함수와 훈련 데이터에 관한 관계를 정리하면,  \n",
    "훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아낸다.\n",
    "\n",
    "이렇게 하려면 모든 훈련 데이터를 대상으로 손실 함수 값을 구해야 한다. 훈련 데이터 100개가 있으면 그로부터 계산한 100개의 손실 함수 값들의 합을 지표로 삼는다는 것.\n",
    "\n",
    "### N개 데이터에 대한 Cross Entropy\n",
    "<img src=\"img/deep_learning_images/e_4.3.png\" width=320 height=320>\n",
    "\n",
    "$t_{nk}$는 n번째 데이터의 k번째 값.(정답 레이블)  \n",
    "$y_{nk}$는 신경망의 출력.  \n",
    "N으로 나눔으로써 정규화, 평균 손실 함수를 구하는 것이다.\n",
    "\n",
    "### 미니 배치 학습을 추가.\n",
    "MNIST의 경우 학습 데이터가 60000개였다. 모든 데이터를 대상으로 손실 함수의 합을 구하려면 시간이 걸린다.  \n",
    "이 많은 데이터를 대상으로 일일이 손실함수를 계산하는 것은 현실적이지 못하므로, 데이터의 일부를 추려 전체의 근사치로 이용.  \n",
    "60000장의 훈련 데이터 중 100장을 무작위로 뽑아 이용하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3963b19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10)\n",
      "(10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from mnist import load_mnist\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_mnist(pickle_path=\"/data/Datasets/MNIST/mnist.pkl\")\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f12c01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([59584, 43715, 27071, 20377, 19731, 49639, 19382, 14049,  2706,\n",
       "       25875])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이 중 10장만 무작위로 빼내려면\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(x_train.shape[0], batch_size)\n",
    "batch_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd6b866e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784) (10, 10)\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x_batch = x_train[batch_mask]\n",
    "y_batch = y_train[batch_mask]\n",
    "\n",
    "print(x_batch.shape, y_batch.shape)\n",
    "print(y_batch[0])\n",
    "print(y_batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06a8b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6da2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
